{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bd8d56e08d441799f6a9ef74e68941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你怎么了？是不是有什么不对劲儿啊？\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# 模型和LoRA权重路径\n",
    "model_path = '/root/autodl-tmp/finetuning/models/ChineseAlpacaGroup/llama-3-chinese-8b-instruct'\n",
    "lora_path = './llama3_lora'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# 加载LoRA权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path, config=None)  # 如果有config参数，添加相应配置\n",
    "\n",
    "# from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM, \n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     inference_mode=True, # 训练模式\n",
    "#     r=8, # Lora 秩\n",
    "#     lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "#     lora_dropout=0.1# Dropout 比例\n",
    "# )\n",
    "\n",
    "\n",
    "# 定义对话函数\n",
    "def chat_with_model(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.encode('')[0],\n",
    "        pad_token_id=tokenizer.eos_token_id # 不加这一行会出warning\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "prompt = \"孙答应和狂徒好上了\"\n",
    "print(chat_with_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "臣妾谨向皇上请安，皇上万福金安。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"你如何称呼我\"\n",
    "print(chat_with_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992b9ede00d641caad5fcfc06aa56e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是甄嬛，家父是大理寺少卿甄远道。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "# 对甄嬛模型进行推理\n",
    "# 模型和LoRA权重路径\n",
    "model_path = '/root/autodl-tmp/finetuning/llama3_zhenhuan'\n",
    "\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# 定义对话函数\n",
    "def chat_with_model(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.encode('')[0],\n",
    "        pad_token_id=tokenizer.eos_token_id # 不加这一行会出warning\n",
    "    )\n",
    "    \n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return response\n",
    "prompt = \"你是谁，介绍一下你自己\"\n",
    "print(chat_with_model(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
